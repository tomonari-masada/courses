{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hiroshima_Univ_Topic_Modeling_HandsOn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/tomonari-masada/course-nlp2020/blob/master/11_topic_modeling.ipynb",
      "authorship_tag": "ABX9TyNn6n3pBIaMUaTqE9L+gIhG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/courses/blob/master/Hiroshima_Univ_Topic_Modeling_HandsOn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahua40er5ilL"
      },
      "source": [
        "# 第1回AI・データイノベーションセミナー 2021年3月11日（木曜日）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPj0E52Uf-se"
      },
      "source": [
        "## トピックモデリングの実践\n",
        "\n",
        "* トピックモデリングを、NMF(nonnegative matrix factorization)とLDA(latent Dirichlet allocation)とで実践してみる。\n",
        "* いずれもscikit-learnの実装を使う。\n",
        "* 各トピックの上位単語はワードクラウドで可視化する。\n",
        "\n",
        "* 参考資料\n",
        " * https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tt8ZFwB5pFt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGYHQvxffZW"
      },
      "source": [
        "## 01 データセットの準備\n",
        "* NeurIPSで発表された1,740本の論文の本文を使う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96F3VXGWfypB"
      },
      "source": [
        "### データをダウンロードしリスト化する関数を定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zZnZWRI6JdY"
      },
      "source": [
        "* PATHで指定した場所に文書ファイルが配置される。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVxhV6BRfalH"
      },
      "source": [
        "import io\n",
        "import os.path\n",
        "import re\n",
        "import tarfile\n",
        "import smart_open\n",
        "\n",
        "\n",
        "PATH = './' # ここは適当に設定\n",
        "\n",
        "\n",
        "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
        "\n",
        "  fname = os.path.join(PATH, url.split('/')[-1])\n",
        "\n",
        "  if not os.path.isfile(fname):\n",
        "    with smart_open.open(url, \"rb\") as fin:\n",
        "      with smart_open.open(fname, 'wb') as fout:\n",
        "        while True:\n",
        "          buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
        "          if not buf:\n",
        "            break\n",
        "          fout.write(buf)\n",
        "\n",
        "  with tarfile.open(fname, mode='r:gz') as tar:\n",
        "  # Ignore directory entries, as well as files like README, etc.\n",
        "    files = [\n",
        "             m for m in tar.getmembers()\n",
        "             if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
        "             ]\n",
        "    for member in sorted(files, key=lambda x: x.name):\n",
        "      member_bytes = tar.extractfile(member).read()\n",
        "      yield member_bytes.decode('utf-8', errors='replace')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkurnJBsf7RX"
      },
      "source": [
        "* 実際にデータを取得しリスト化する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taMYgFMor-j_"
      },
      "source": [
        "docs = list(extract_documents())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPSjQ6PNgBK1"
      },
      "source": [
        "* 文書数、具体的な文書の内容などを確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhY0Iq0ycIv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1869bcfc-ca8d-4576-9c29-ac87cc467e4b"
      },
      "source": [
        "print(len(docs))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSTVJr5jgC5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd1401f-ccb4-4e3c-9d76-9fd0d5afb214"
      },
      "source": [
        "print(docs[0][:1000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \n",
            "CONNECTIVITY VERSUS ENTROPY \n",
            "Yaser S. Abu-Mostafa \n",
            "California Institute of Technology \n",
            "Pasadena, CA 91125 \n",
            "ABSTRACT \n",
            "How does the connectivity of a neural network (number of synapses per \n",
            "neuron) relate to the complexity of the problems it can handle (measured by \n",
            "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
            "functions can be implemented using a circuit with very low connectivity (e.g., \n",
            "using two-input NAND gates). However, for a network that learns a problem \n",
            "from examples using a local learning rule, we prove that the entropy of the \n",
            "problem becomes a lower bound for the connectivity of the network. \n",
            "INTRODUCTION \n",
            "The most distinguishing feature of neural networks is their ability to spon- \n",
            "taneously learn the desired function from 'training' samples, i.e., their ability \n",
            "to program themselves. Clearly, a given neural network cannot just learn any \n",
            "function, there must be some restrictions on which networks can learn which \n",
            "functions. One obv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM4WRXtQgL9G"
      },
      "source": [
        "### spaCyを使ってtokenizeする\n",
        "* spaCyについては https://spacy.io/ を参照"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZZGGkqEgQ94"
      },
      "source": [
        "* 前処理の高速化のためtaggerなどは無効にしておく"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi6mRRyjgJq_"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFKq6fvQhNTg"
      },
      "source": [
        "* テキストを小文字にしてからtokenizeする関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LVnUt6FgYbW"
      },
      "source": [
        "def spacy_lemmatize_text(nlp, text):\n",
        "  text = nlp(text.lower())\n",
        "  doc = [word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text]\n",
        "  return [word for word in doc if len(word) > 1] # 長さ1の単語は削除"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPvZBojxgYcW"
      },
      "source": [
        "* tokenizationの実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iICBt6EgqC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d9b389-cf07-47a6-e4c4-0e1736b838e6"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "new_docs = list()\n",
        "for doc in tqdm(docs):\n",
        "  new_docs.append(spacy_lemmatize_text(nlp, doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 51%|█████     | 882/1740 [00:30<00:31, 27.66it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBDnVSoZgaMJ"
      },
      "source": [
        "* tokenizationの結果を確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6MVn-YGhBst"
      },
      "source": [
        "print(new_docs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7lABQFJgdBj"
      },
      "source": [
        "* 各文書を長い文字列で表しなおす（後でCountVectorizerを使うため）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnNRbzLzi6vh"
      },
      "source": [
        "corpus = [' '.join(doc) for doc in new_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-72RddHDIME"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nu0eT7OXCzD"
      },
      "source": [
        "## 02 データ行列の作成\n",
        "* NMFの場合、TF-IDFで単語の重みを求めて各文書をベクトル化する。\n",
        "* LDAの場合、単に単語の出現頻度を重みとして各文書をベクトル化する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo8oPlaMCzoM"
      },
      "source": [
        "### sklearnのCountVectorizerで疎行列化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pyLBUfSXTUL"
      },
      "source": [
        "* 全文書の半分より多い文書に現れる単語は、高頻度語とみなして削除する。\n",
        "* 10件未満の文書にしか現れない単語は、低頻度語とみなして削除する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrj0mmMrCzNI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.5, min_df=10, stop_words='english')\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diy077KvDdxi"
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LVXuO9GDysO"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoE0bBHJEFEj"
      },
      "source": [
        "print(len(vectorizer.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2wVko9vXnlq"
      },
      "source": [
        "* 文書数と語彙サイズを変数にセット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZYC-cLaFV9V"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAd821DGFXXp"
      },
      "source": [
        "n_samples, n_features = X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG8puM9OXrNk"
      },
      "source": [
        "### TF-IDFで各文書における単語の重みを計算する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywX2HtW-Elar"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer()\n",
        "Xtfidf = tfidf.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdac5_tSE4YC"
      },
      "source": [
        "print(Xtfidf[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0mRZonFEJF"
      },
      "source": [
        "Xtfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdJTl_VYNHV"
      },
      "source": [
        "* 抽出するトピックの個数は、今回は20個とする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjCUJPTsYM5H"
      },
      "source": [
        "n_components = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_CT7nbdUtVX"
      },
      "source": [
        "## 03 NMFでトピック抽出\n",
        "* まず、TF-IDFのデータ行列を使って　NMFによってトピック抽出を試みる。\n",
        " * NMFのパラメータ群は下記サンプルコードを参考にした。\n",
        " * https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXUdhYDMYuDO"
      },
      "source": [
        "### NMFとLDAのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L34qQ1iFncJ"
      },
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAVXaQM0Y2Tz"
      },
      "source": [
        "### NMFによるトピック抽出の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD5YYXTmFhBj"
      },
      "source": [
        "from time import time\n",
        "\n",
        "print((f\"Fitting the NMF model (generalized Kullback-Leibler \"\n",
        "  f\"divergence) with tf-idf features, n_samples={n_samples} \"\n",
        "  f\"and n_features={n_features}\"))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1, l1_ratio=.5,\n",
        "          verbose=1)\n",
        "nmf.fit(Xtfidf)\n",
        "print(f\"done in {time() - t0:0.3f}s.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkg5KZoHYYCN"
      },
      "source": [
        "* NMFにおける各コンポーネントは、それぞれのトピックにおける単語の重要度を表すベクトルとして表現されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve9R8yHfG4AM"
      },
      "source": [
        "nmf.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPHsNjupYp7w"
      },
      "source": [
        "### トピックの重要語を取り出す関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpoC-pofHMEO"
      },
      "source": [
        "def get_top_words(model, feature_names, n_top_words=30):\n",
        "  top_features = list()\n",
        "  weights = list()\n",
        "  for topic_idx, topic in enumerate(model.components_):\n",
        "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
        "    top_features.append([feature_names[i] for i in top_features_ind])\n",
        "    weights.append(topic[top_features_ind])\n",
        "  return top_features, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8K-_A0AZIZ0"
      },
      "source": [
        "### NMFの各コンポーネントから重要語を取り出す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDe-I2RuSvMu"
      },
      "source": [
        "top_words, weights = get_top_words(nmf, vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJsNT2kUULwl"
      },
      "source": [
        "print(top_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN10-rDQU2rt"
      },
      "source": [
        "topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75wn85J2U6gj"
      },
      "source": [
        "topic_words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWbQ_eRpZQNs"
      },
      "source": [
        "### 重要語をワードクラウドで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxUXK24EkgWy"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W103IkR3ZWYg"
      },
      "source": [
        "* ワードクラウドから除去するストップワードを確認する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0p-1tK7k7rk"
      },
      "source": [
        "print(STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INZcGJAbF_o"
      },
      "source": [
        "* ワードクラウドを描画"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRWtPdcOkwiG"
      },
      "source": [
        "cloud = WordCloud(stopwords=STOPWORDS,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=100,\n",
        "                  colormap='tab10'\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmbw1bOHkhfN"
      },
      "source": [
        "fig, axes = plt.subplots(5, 4, figsize=(16, 25), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  cloud.generate_from_frequencies(topic_words[i], max_font_size=300)\n",
        "  plt.gca().imshow(cloud)\n",
        "  plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "  plt.gca().axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EHfuc6RZgPh"
      },
      "source": [
        "## 04 scikit-learnのLDAでトピック抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAEThMwZjhb"
      },
      "source": [
        "### LDAによるトピック抽出の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuuSTK6fZfu7"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=n_components, \n",
        "                                max_iter=20,\n",
        "                                topic_word_prior=0.01,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50,\n",
        "                                batch_size=200,\n",
        "                                mean_change_tol=1e-4,\n",
        "                                random_state=1,\n",
        "                                evaluate_every=1,\n",
        "                                verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWd3DND8aAVz"
      },
      "source": [
        "print((f\"Fitting LDA models with tf features, \"\n",
        "  f\"n_samples={n_samples} and n_features={n_features}\"))\n",
        "t0 = time()\n",
        "lda.fit(X)\n",
        "print(f\"done in {time() - t0:0.3f}s.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pez5Ak2-cUjS"
      },
      "source": [
        "### LDAの各トピックから高確率語を取り出す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBI2nx6aEri"
      },
      "source": [
        "top_words, weights = get_top_words(lda, vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HADZSPDcJbC"
      },
      "source": [
        "print(top_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQgOfcGYb8gA"
      },
      "source": [
        "topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_HRjNZJcbNr"
      },
      "source": [
        "### 高確率語をワードクラウドで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o3v2CLfb1ku"
      },
      "source": [
        "cloud = WordCloud(stopwords=STOPWORDS,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=100,\n",
        "                  colormap='tab10'\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFf5jcX5b45d"
      },
      "source": [
        "fig, axes = plt.subplots(5, 4, figsize=(16, 25), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  cloud.generate_from_frequencies(topic_words[i], max_font_size=300)\n",
        "  plt.gca().imshow(cloud)\n",
        "  plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "  plt.gca().axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd-AV6Yz8S5K"
      },
      "source": [
        "## pyLDAvisという可視化ツールでトピックを可視化\n",
        "* https://pyldavis.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09yuBwmocMRn"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEINkILeopMw"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KGlybhAosnv"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda, X, vectorizer, mds='tsne')\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOlH0IuE7nWi"
      },
      "source": [
        "## 05 gensimのLDAでトピック抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4waWcUD7mHo"
      },
      "source": [
        "* gensimのLdaModelはデフォルトの設定だと正しく動かない\n",
        " * passesを20ぐらいにはしておくこと。\n",
        " * 下記Webページは使い方を間違っているので要注意（passesをデフォルト設定で使っている）\n",
        " http://www.ie110704.net/2018/12/29/wordcloud%E3%81%A8pyldavis%E3%81%AB%E3%82%88%E3%82%8Blda%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/\n",
        "* gensimのperplexityはトークンあたりのELBOのnp.exp2()で求めている\n",
        " * 自然対数の底を使って求めたELBOをもとにして計算しているにもかかわらず。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu7i2siF7q99"
      },
      "source": [
        "from gensim import corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8nyWJjz7yiI"
      },
      "source": [
        "dictionary = corpora.Dictionary(new_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0MUXeJF72Qo"
      },
      "source": [
        "print(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sea-Qc7j73mx"
      },
      "source": [
        "dictionary.filter_extremes(no_below=10, no_above=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EDAR5yZ7472"
      },
      "source": [
        "len(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdNNK06G76fM"
      },
      "source": [
        "gs_corpus = [dictionary.doc2bow(doc) for doc in new_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys69pRnE7_U6"
      },
      "source": [
        "print(gs_corpus[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8169JeN8G65"
      },
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='myapp.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFok0rPg8JGU"
      },
      "source": [
        "from gensim.models.ldamodel import LdaModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4do_yx7H8Khu"
      },
      "source": [
        "lda = LdaModel(corpus=gs_corpus, num_topics=n_components,\n",
        "               passes=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_5oqOx8M-n"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.exp(- lda.log_perplexity(gs_corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3d4qtw-_duF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}